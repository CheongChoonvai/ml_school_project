{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36111a42",
   "metadata": {},
   "source": [
    "# Cancer Data Exploration Notebook\n",
    "\n",
    "This notebook provides exploratory data analysis of the Global Cancer Patients dataset used in the OncoPredictAI project. We'll analyze key features, distributions, and relationships to gain insights for model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765c5fe",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's set up the environment and import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e0ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root directory to the path\n",
    "project_root = Path('..', '..').resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load project configuration\n",
    "config_path = os.path.join(project_root, 'config', 'default.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set visualization style\n",
    "from src.visualization.visualize import set_visualization_style\n",
    "viz_config = config.get('visualization', {})\n",
    "set_visualization_style(\n",
    "    style=viz_config.get('style', 'whitegrid'),\n",
    "    context=viz_config.get('context', 'notebook'),\n",
    "    palette=viz_config.get('palette', 'viridis')\n",
    ")\n",
    "\n",
    "print(f\"OncoPredictAI Project directory: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341dd45",
   "metadata": {},
   "source": [
    "## 2. Load and Examine the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06eeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the cancer dataset\n",
    "data_path = os.path.join(project_root, 'data', 'raw', 'cancer_patients', 'global_cancer_patients_2015_2024.csv')\n",
    "\n",
    "# Check if file exists, if not, check in the original location\n",
    "if not os.path.exists(data_path):\n",
    "    data_path = os.path.join(project_root, 'data', 'global_cancer_patients_2015_2024.csv')\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset shape: {df.shape} (rows, columns)\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81271e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing,\n",
    "    'Percent': missing_percent\n",
    "}).sort_values('Missing Values', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Values'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1a79a",
   "metadata": {},
   "source": [
    "## 3. Statistical Summary\n",
    "\n",
    "Let's examine the statistical distribution of the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22920d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609a545",
   "metadata": {},
   "source": [
    "## 4. Categorical Data Analysis\n",
    "\n",
    "Now let's examine the categorical variables and their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Display value counts for each categorical column\n",
    "for col in categorical_cols[:5]:  # Limit to first 5 to avoid too much output\n",
    "    print(f\"\\nDistribution of {col}:\")\n",
    "    print(df[col].value_counts().sort_values(ascending=False).head(10))\n",
    "    print(f\"Number of unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37288b53",
   "metadata": {},
   "source": [
    "## 5. Target Variable Analysis\n",
    "\n",
    "Let's explore the target variable (severity score) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dffe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Severity_Score'  # Adjust based on actual column name\n",
    "\n",
    "if target_col in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[target_col], kde=True)\n",
    "    plt.title(f'Distribution of {target_col}')\n",
    "    plt.xlabel(target_col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\n{target_col} statistics:\")\n",
    "    print(df[target_col].describe())\n",
    "else:\n",
    "    print(f\"Target column '{target_col}' not found in dataset.\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda38cbc",
   "metadata": {},
   "source": [
    "## 6. Feature Correlations\n",
    "\n",
    "Let's examine correlations between numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = df[numeric_cols].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title('Feature Correlation Matrix', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b1643",
   "metadata": {},
   "source": [
    "## 7. Feature Distributions\n",
    "\n",
    "Let's visualize distributions of key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 6 most important numeric features (excluding target if present)\n",
    "features_to_plot = [col for col in numeric_cols if col != target_col][:6]\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    sns.histplot(df[feature], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db70f17",
   "metadata": {},
   "source": [
    "## 8. Relationship Between Key Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14754ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col in df.columns:\n",
    "    # Select top 3 features with highest correlation to target\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr_with_target = corr[target_col].abs().sort_values(ascending=False)\n",
    "        top_features = corr_with_target.index[1:4]  # Skip the target itself\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        for i, feature in enumerate(top_features):\n",
    "            sns.scatterplot(x=feature, y=target_col, data=df, alpha=0.6, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature} vs {target_col}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough numeric columns for correlation analysis\")\n",
    "else:\n",
    "    print(f\"Target column '{target_col}' not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ce5f9",
   "metadata": {},
   "source": [
    "## 9. Initial Clustering Analysis\n",
    "\n",
    "Let's apply K-means clustering to identify potential patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.clustering.kmeans import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features for clustering (excluding target)\n",
    "features = [col for col in numeric_cols if col != target_col]\n",
    "features = features[:10]  # Limit to top 10 features for performance\n",
    "\n",
    "# Prepare data\n",
    "X = df[features].copy()\n",
    "\n",
    "# Handle missing values if any\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-means with different cluster numbers to determine optimal k\n",
    "k_values = range(2, 8)\n",
    "inertias = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, inertias, 'o-', linewidth=2)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04203843",
   "metadata": {},
   "source": [
    "## 10. Dimensionality Reduction with PCA\n",
    "\n",
    "Apply PCA to reduce dimensionality and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ad793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dimensionality_reduction.pca import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot explained variance\n",
    "pca.plot_explained_variance()\n",
    "plt.show()\n",
    "\n",
    "# Apply K-means on PCA results\n",
    "optimal_k = 3  # Use result from elbow curve\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Plot the clusters in 2D PCA space\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.7, s=50)\n",
    "plt.scatter(pca.transform(kmeans.centroids), marker='X', s=200, c='red', label='Centroids')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Clusters visualized in PCA space')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088c395",
   "metadata": {},
   "source": [
    "## 11. Summary of Findings\n",
    "\n",
    "In this exploratory analysis, we've discovered several key insights from the Global Cancer Patients dataset:\n",
    "\n",
    "1. **Dataset Overview**: [Summarize the size and scope of the dataset]\n",
    "2. **Missing Values**: [Summarize findings about missing data]\n",
    "3. **Feature Correlations**: [Highlight key correlations discovered]\n",
    "4. **Target Variable**: [Describe distribution of target variable]\n",
    "5. **Clustering Results**: [Describe what the clustering analysis revealed]\n",
    "6. **PCA Results**: [Summarize dimensionality reduction findings]\n",
    "\n",
    "These insights will inform our feature engineering and model selection process for the OncoPredictAI system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ef35a",
   "metadata": {},
   "source": [
    "## 12. Next Steps\n",
    "\n",
    "Based on this exploratory analysis, we'll proceed with:\n",
    "\n",
    "1. Feature engineering to enhance predictive power\n",
    "2. Advanced preprocessing techniques to handle identified data quality issues\n",
    "3. Model development focusing on [specific algorithms identified as promising]\n",
    "4. Detailed evaluation of model performance for cancer prediction tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
